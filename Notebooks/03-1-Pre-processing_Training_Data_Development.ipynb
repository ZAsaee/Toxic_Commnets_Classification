{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a63c4e",
   "metadata": {},
   "source": [
    "#  Pre-processing & Training Data Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fc4fa",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be1502",
   "metadata": {},
   "source": [
    "[1. Introduction](#1.-Introduction)\n",
    "<br>[2. Import Libraries and Data](#2.-Import-Libraries-and-Data)\n",
    "<br>[3. Evaluate the Text Vectorization Algorithms](#3.-Evaluate-the-Text-Vectorization-Algorithms)\n",
    "<br>&emsp;&emsp;&emsp;[3.1. BOW Term Frequency](#3.1.-BOW-Term-Frequency)\n",
    "<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.1. Initial Exploration](#3.1.1.-Initial-Exploration)\n",
    "<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.1.2. BOW Term Frequency - Parameters Evaluation](#3.1.2.-BOW-Term-Frequency---Parameters-Evaluation)\n",
    "<br>&emsp;&emsp;&emsp;[3.2. Normalized TF-IDF](#3.2.-Normalized-TF-IDF)\n",
    "<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.2.1. Initial Exploration](#3.2.1.-Initial-Exploration)\n",
    "<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[3.2.2. TF-IDF Vectorization - Parameters Evaluation](#3.2.2.-TF-IDF-Vectorization---Parameters-Evaluation)\n",
    "<br>&emsp;&emsp;&emsp;[3.3. Summary of Text Vecorization](#3.3.-Summary-of-Text-Vecorization)\n",
    "<br>[4. Evaluate Imbalanced Algorithm](#4.-Evaluate-Imbalanced-Algorithm)\n",
    "<br>&emsp;&emsp;&emsp;[4.1. Oversampling Algorithms](#4.1.-Oversampling-Algorithms)\n",
    "<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[4.1.1. Random Oversampling](#4.1.1.-Random-Oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde456a",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "Classification predictive modelling is a category of machine learning problems where a class label is assigned to a given dataset. The main types of classification problems are binary classification, multi-class classification, multi-label classification and imbalanced classification. The present project is a binary imbalanced classification problem with the following labels. The labels are “toxic” and “non-toxic”. The distribution of comments in these two labels is not uniformly distributed, therefore it is an imbalanced classification.\n",
    "\n",
    "    \n",
    "|  | <b>Label | <b>Number of Comments |\n",
    "| :- | :-: | :-: |\n",
    "|1| Non-toxic | 143346 |\n",
    "|2| Toxic | 16225 |\n",
    "\n",
    "   The imbalanced classification is a challenging problem. It requires a high-level framework for systematically handling the skewed class distribution. In this project, I follow the below systematic framework to evaluate various methods of classification:\n",
    "    \n",
    "#### 1. Select a performance metric\n",
    "    \n",
    "One of the main steps for the evaluation of the model is selecting the right metric. There are mainly two factors for the choice of classification: imbalanced or balanced dataset, and the business use-case to solve. In the case of imbalanced classification, selecting the appropriate metric is more challenging. Since most of the widely used standard metrics assume a balanced class distribution. For example, classification accuracy is one of the most common standard metrics for the classification problem. However, this metric for the imbalanced classification problem is very dangerous and misleading. \n",
    "There are four types of outcomes for the classification prediction: True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN). Four performance metrics based on these outcomes are defined as follows:\n",
    " \n",
    "\n",
    "\\begin{gather*}\n",
    "Accuray = \\frac {TP + TN} {TP + FP + TN + FN}\n",
    "\\end{gather*}\n",
    "  \n",
    "    \n",
    "\\begin{gather*}\n",
    "Precision = \\frac {TP} {TP + FP}\n",
    "\\end{gather*}\n",
    "  \n",
    "    \n",
    "\\begin{gather*}\n",
    "Recall = \\frac {TP} {TP + FN}\n",
    "\\end{gather*}\n",
    "  \n",
    "    \n",
    "\\begin{gather*}\n",
    "F1 = \\frac {2 * Precision * Recall} {Precision + Recall}\n",
    "\\end{gather*}\n",
    "    \n",
    "For binary imbalanced classification tasks, the majority class is normal (called the “negative class”), and the minority class is the exception (called the “positive class”). As mentioned earlier, the accuracy metric is not an appropriate choice for imbalanced classification problems. Based on the level of tolerance with respect to false positives and false negatives, the right performance metric can be selected.\n",
    "In the current study, the assumption is that a social media platform is willing to detect toxic comments and block the corresponding accounts. Therefore, it is essential to decrease false positives, since we don’t want to block an account by mistake (high precision). On the other hand, it is important to detect toxic comments and block them (high recall). In practice, we can’t achieve both high precision and high recall. An increase in precision metric reduces recall metric and vice versa. This is called the precision/recall tradeoff. Therefore, the F1 score is better metric performance when we are seeking a balance between precision and recall. \n",
    "Moreover, the receiver operator characteristic (ROC/AUC curve) is another useful metric for classification evaluation. For classification problems with probability outputs, a threshold can be used for the classification. ROC plots out the sensitivity and specificity for the different thresholds and possible outputs. \n",
    "\n",
    "\\begin{gather*}\n",
    "Sensitivity = \\frac {TP} {TP + FN}\n",
    "\\end{gather*}\n",
    "    \n",
    "\\begin{gather*}\n",
    "Specifity = \\frac {FP} {FP + TN}\n",
    "\\end{gather*}\n",
    "\n",
    "In the current study, the performance metric to evaluate the models are F1 score and ROC/AUC.\n",
    "\n",
    "#### 2. Evaluate the text vectorization algorithms\n",
    "\n",
    "The text vectorization algorithms used in this project are as follows:\n",
    " - Bag of Words (BOW) Term Frequency\n",
    " - Normalized TF-IDF\n",
    "\n",
    "#### 3. Evaluate classification algorithms\n",
    "I evaluate the performance of various classification algoritms as follows:\n",
    " - Logistic Regression\n",
    " - Random Forest Classifier\n",
    " - XGBOOST Classifier\n",
    " - LGBM Classifier\n",
    " - Naive Bayes\n",
    " - KNN\n",
    "\n",
    "#### 4. Evaluate imbalanced algorithms\n",
    "The performance of various undersampling and oversampling methods are evaluated as follows:\n",
    " - Random Oversampling\n",
    " - Synthetic Minority Oversampling (SMOTE)\n",
    " - Adaptive Synthetic Sampling (ADASYN)\n",
    " - Borderline SMOTE\n",
    " - Random Undersampling\n",
    " - NearMiss\n",
    " - Edited Nearest Neighbor\n",
    "    \n",
    "#### 5. Hyperparameter tuning\n",
    "GridSearchCV and BayesSearchCV are applied to select the suitable hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92408f",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d81150da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Model selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Resampling techniques selection\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('max_colwidth', 150)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e09be6",
   "metadata": {},
   "source": [
    "In the preceding notebook, text vectorization and resampling techniques are examined. The initial model are trained using Logistic Regression model. Two techniques of Countvectorizer and TF-IDF are employed to vectorize the text. Moreover, several undersampling and oversampling techniques and their combinations are examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ce18e424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>toxic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edit make username hardcore metallica fan revert vandalisms closure gas vote new york dolls fac please remove template talk page since...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww match background colour seemingly stuck thank talk january utc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man really not try edit war guy constantly remove relevant information talk edit instead talk page seem care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make real suggestion improvement wonder section statistic later subsection type accident think reference may need tidy exact format ie date format...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              clean_text  toxic_type\n",
       "0  explanation edit make username hardcore metallica fan revert vandalisms closure gas vote new york dolls fac please remove template talk page since...           0\n",
       "1                                                                                     aww match background colour seemingly stuck thank talk january utc           0\n",
       "2                hey man really not try edit war guy constantly remove relevant information talk edit instead talk page seem care formatting actual info           0\n",
       "3  make real suggestion improvement wonder section statistic later subsection type accident think reference may need tidy exact format ie date format...           0\n",
       "4                                                                                                                          sir hero chance remember page           0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Library/cleaned_text_train_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce2ba140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    54\n",
       "toxic_type     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4fc13158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ec00b",
   "metadata": {},
   "source": [
    "## 3. Evaluate the Text Vectorization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ffc4d3",
   "metadata": {},
   "source": [
    "The main goal is to investigate the text vectorization algorithms and find out the optimum parameters. Initially, the models are tested with the default values. Afterward, GridSearch technique is employed to investigate the effects of parameters on the model.\n",
    "First, I split the dataset to train/test set and then fit the text vectorization model on the training set. Afterwards, both train and test are transformed using this fit. It is important to perform the split first to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66564fd5",
   "metadata": {},
   "source": [
    "### 3.1. BOW Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6a927",
   "metadata": {},
   "source": [
    "### 3.1.1. Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c879eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X = df['clean_text']\n",
    "y = df['toxic_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "411b7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to perform train/test split and then fitting and transforming using Countvectorizer\n",
    "\n",
    "def countvectorize(X, y, max_df=0.95, min_df=0.001, ngram_range=(1,1)):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                                stratify=y, random_state=1)\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, ngram_range=(1,1))\n",
    "    Dict_count = vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train_vec = Dict_count.transform(X_train).toarray()\n",
    "    X_test_vec = Dict_count.transform(X_test).toarray()\n",
    "    \n",
    "    X_train_df = pd.DataFrame(data=X_train_vec, columns=vectorizer.get_feature_names())\n",
    "    X_test_df = pd.DataFrame(data=X_test_vec, columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    y_train_df = y_train\n",
    "    y_test_df = y_test\n",
    "    \n",
    "    print('Number of labels in the train set: \\n', y_train.value_counts(normalize=True))\n",
    "    print('-----------------------------------')\n",
    "    print('Number of labels in the test set: \\n', y_test.value_counts(normalize=True))\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    X_train = np.array(X_train_df)\n",
    "    X_test = np.array(X_test_df)\n",
    "    y_train = np.array(y_train_df)\n",
    "    y_test = np.array(y_test_df)\n",
    "    \n",
    "    print(\"X train type and shape: \", type(X_train), X_train.shape)\n",
    "    print(\"y train type and shape: \", type(y_train), y_train.shape)\n",
    "    \n",
    "    print(\"X test type and shape: \", type(X_test), X_test.shape)\n",
    "    print(\"y test type and shape: \", type(y_test), y_test.shape)\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520ef8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in the train set: \n",
      " 0    0.898286\n",
      "1    0.101714\n",
      "Name: toxic_type, dtype: float64\n",
      "-----------------------------------\n",
      "Number of labels in the test set: \n",
      " 0    0.898289\n",
      "1    0.101711\n",
      "Name: toxic_type, dtype: float64\n",
      "-----------------------------------\n",
      "X train type and shape:  <class 'numpy.ndarray'> (127613, 2958)\n",
      "y train type and shape:  <class 'numpy.ndarray'> (127613,)\n",
      "X test type and shape:  <class 'numpy.ndarray'> (31904, 2958)\n",
      "y test type and shape:  <class 'numpy.ndarray'> (31904,)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# BOW Word Frequency\n",
    "\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = countvectorize(X, y, max_df=0.95, min_df=0.001, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26bfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=20000, random_state=1)\n",
    "clf_model = clf.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "y_pred_train_bow = clf_model.predict(X_train_bow)\n",
    "\n",
    "y_pred_test_bow = clf_model.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9e143",
   "metadata": {},
   "source": [
    "I check the classification report for both train and test sets to check underfitting or overfitting of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc19aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     28659\n",
      "           1       0.86      0.64      0.74      3245\n",
      "\n",
      "    accuracy                           0.95     31904\n",
      "   macro avg       0.91      0.82      0.86     31904\n",
      "weighted avg       0.95      0.95      0.95     31904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the classification metrics for the test set\n",
    "\n",
    "print(\"Test Classification Report\")\n",
    "print(classification_report(y_test_bow, y_pred_test_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f65938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    114633\n",
      "           1       0.91      0.66      0.76     12980\n",
      "\n",
      "    accuracy                           0.96    127613\n",
      "   macro avg       0.93      0.83      0.87    127613\n",
      "weighted avg       0.96      0.96      0.96    127613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the classification metrics for the train set\n",
    "\n",
    "print(\"Test Classification Report\")\n",
    "print(classification_report(y_train_bow, y_pred_train_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82231ea",
   "metadata": {},
   "source": [
    "As mentioned earlier, the classification metric for this study is f1-score; however, I keep checking precision and recall, as well. \n",
    "\n",
    "In terms of minority group (toxic comments), the initial model has a higher precision than recall. It was expected!\n",
    "The goal is to improve recall and thus f1-score.\n",
    "\n",
    "In the next section, I investiagte the effects of parameters on the output of BOW model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7824e14",
   "metadata": {},
   "source": [
    "### 3.1.2. BOW Term Frequency - Parameters Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df['toxic_type']\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c91808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BOW and Logistic Regression models with different parameters\n",
    "\n",
    "pipeline = Pipeline([('countvect', CountVectorizer()),\n",
    "                     ('clf', LogisticRegression(solver='lbfgs', max_iter=20000))])\n",
    "\n",
    "parameters = {\n",
    "    'countvect__max_df': (0.95, 0.9, 0.85, 0.8),\n",
    "    'countvect__min_df': (0.0001, 0.0005, 0.001, 0.002),\n",
    "    'countvect__ngram_range': ((1,1), (2,2))\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='f1')\n",
    "model_bow = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b54a392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_countvect__max_df</th>\n",
       "      <th>param_countvect__min_df</th>\n",
       "      <th>param_countvect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.432982</td>\n",
       "      <td>1.866859</td>\n",
       "      <td>0.662657</td>\n",
       "      <td>0.068872</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.750159</td>\n",
       "      <td>0.764893</td>\n",
       "      <td>0.764932</td>\n",
       "      <td>0.745038</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.756486</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.090825</td>\n",
       "      <td>1.215975</td>\n",
       "      <td>0.566545</td>\n",
       "      <td>0.005077</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.750159</td>\n",
       "      <td>0.764893</td>\n",
       "      <td>0.764932</td>\n",
       "      <td>0.745038</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.756486</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.839809</td>\n",
       "      <td>1.022437</td>\n",
       "      <td>0.558752</td>\n",
       "      <td>0.007119</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.750159</td>\n",
       "      <td>0.764893</td>\n",
       "      <td>0.764932</td>\n",
       "      <td>0.745038</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.756486</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.854013</td>\n",
       "      <td>1.047872</td>\n",
       "      <td>0.566128</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.750159</td>\n",
       "      <td>0.764893</td>\n",
       "      <td>0.764932</td>\n",
       "      <td>0.745038</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>0.756486</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.173404</td>\n",
       "      <td>1.001998</td>\n",
       "      <td>0.549761</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.737915</td>\n",
       "      <td>0.755251</td>\n",
       "      <td>0.748811</td>\n",
       "      <td>0.730634</td>\n",
       "      <td>0.742808</td>\n",
       "      <td>0.743084</td>\n",
       "      <td>0.008516</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.149528</td>\n",
       "      <td>1.013364</td>\n",
       "      <td>0.542664</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.737915</td>\n",
       "      <td>0.755251</td>\n",
       "      <td>0.748811</td>\n",
       "      <td>0.730634</td>\n",
       "      <td>0.742808</td>\n",
       "      <td>0.743084</td>\n",
       "      <td>0.008516</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.699656</td>\n",
       "      <td>1.057683</td>\n",
       "      <td>0.618047</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.737915</td>\n",
       "      <td>0.755251</td>\n",
       "      <td>0.748811</td>\n",
       "      <td>0.730634</td>\n",
       "      <td>0.742808</td>\n",
       "      <td>0.743084</td>\n",
       "      <td>0.008516</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.169474</td>\n",
       "      <td>1.032815</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.737915</td>\n",
       "      <td>0.755251</td>\n",
       "      <td>0.748811</td>\n",
       "      <td>0.730634</td>\n",
       "      <td>0.742808</td>\n",
       "      <td>0.743084</td>\n",
       "      <td>0.008516</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.220552</td>\n",
       "      <td>1.042784</td>\n",
       "      <td>0.546806</td>\n",
       "      <td>0.009530</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.724054</td>\n",
       "      <td>0.736449</td>\n",
       "      <td>0.735216</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.730004</td>\n",
       "      <td>0.727547</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.159653</td>\n",
       "      <td>1.030859</td>\n",
       "      <td>0.537663</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.724054</td>\n",
       "      <td>0.736449</td>\n",
       "      <td>0.735216</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.730004</td>\n",
       "      <td>0.727547</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.181622</td>\n",
       "      <td>1.009740</td>\n",
       "      <td>0.536921</td>\n",
       "      <td>0.011758</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.724054</td>\n",
       "      <td>0.736449</td>\n",
       "      <td>0.735216</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.730004</td>\n",
       "      <td>0.727547</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.027201</td>\n",
       "      <td>1.151801</td>\n",
       "      <td>0.621795</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.724054</td>\n",
       "      <td>0.736449</td>\n",
       "      <td>0.735216</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.730004</td>\n",
       "      <td>0.727547</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.691958</td>\n",
       "      <td>0.885304</td>\n",
       "      <td>0.525314</td>\n",
       "      <td>0.015363</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.689577</td>\n",
       "      <td>0.711324</td>\n",
       "      <td>0.702912</td>\n",
       "      <td>0.678330</td>\n",
       "      <td>0.701530</td>\n",
       "      <td>0.696734</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.691072</td>\n",
       "      <td>0.874775</td>\n",
       "      <td>0.531409</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.689577</td>\n",
       "      <td>0.711324</td>\n",
       "      <td>0.702912</td>\n",
       "      <td>0.678330</td>\n",
       "      <td>0.701530</td>\n",
       "      <td>0.696734</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.691791</td>\n",
       "      <td>0.731539</td>\n",
       "      <td>0.543389</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.689577</td>\n",
       "      <td>0.711324</td>\n",
       "      <td>0.702912</td>\n",
       "      <td>0.678330</td>\n",
       "      <td>0.701530</td>\n",
       "      <td>0.696734</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.744992</td>\n",
       "      <td>0.865440</td>\n",
       "      <td>0.529615</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.689577</td>\n",
       "      <td>0.711324</td>\n",
       "      <td>0.702912</td>\n",
       "      <td>0.678330</td>\n",
       "      <td>0.701530</td>\n",
       "      <td>0.696734</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.724845</td>\n",
       "      <td>0.351676</td>\n",
       "      <td>0.942195</td>\n",
       "      <td>0.013466</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.362353</td>\n",
       "      <td>0.355993</td>\n",
       "      <td>0.366627</td>\n",
       "      <td>0.368943</td>\n",
       "      <td>0.360851</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.983344</td>\n",
       "      <td>0.402603</td>\n",
       "      <td>0.857127</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.362353</td>\n",
       "      <td>0.355993</td>\n",
       "      <td>0.366627</td>\n",
       "      <td>0.368943</td>\n",
       "      <td>0.360851</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.045657</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.863838</td>\n",
       "      <td>0.016539</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.362353</td>\n",
       "      <td>0.355993</td>\n",
       "      <td>0.366627</td>\n",
       "      <td>0.368943</td>\n",
       "      <td>0.360851</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.975618</td>\n",
       "      <td>0.291010</td>\n",
       "      <td>0.847991</td>\n",
       "      <td>0.017757</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.362353</td>\n",
       "      <td>0.355993</td>\n",
       "      <td>0.366627</td>\n",
       "      <td>0.368943</td>\n",
       "      <td>0.360851</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.184012</td>\n",
       "      <td>0.153570</td>\n",
       "      <td>0.904768</td>\n",
       "      <td>0.012956</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.161554</td>\n",
       "      <td>0.181078</td>\n",
       "      <td>0.163475</td>\n",
       "      <td>0.161092</td>\n",
       "      <td>0.176331</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.430287</td>\n",
       "      <td>0.144647</td>\n",
       "      <td>0.793471</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.161554</td>\n",
       "      <td>0.181078</td>\n",
       "      <td>0.163475</td>\n",
       "      <td>0.161092</td>\n",
       "      <td>0.176331</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.488005</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.804580</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.161554</td>\n",
       "      <td>0.181078</td>\n",
       "      <td>0.163475</td>\n",
       "      <td>0.161092</td>\n",
       "      <td>0.176331</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.478919</td>\n",
       "      <td>0.159535</td>\n",
       "      <td>0.812701</td>\n",
       "      <td>0.021175</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.161554</td>\n",
       "      <td>0.181078</td>\n",
       "      <td>0.163475</td>\n",
       "      <td>0.161092</td>\n",
       "      <td>0.176331</td>\n",
       "      <td>0.168706</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.277324</td>\n",
       "      <td>0.097248</td>\n",
       "      <td>0.792323</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>0.107117</td>\n",
       "      <td>0.093174</td>\n",
       "      <td>0.092319</td>\n",
       "      <td>0.110553</td>\n",
       "      <td>0.100812</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.217672</td>\n",
       "      <td>0.066639</td>\n",
       "      <td>0.790604</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>0.107117</td>\n",
       "      <td>0.093174</td>\n",
       "      <td>0.092319</td>\n",
       "      <td>0.110553</td>\n",
       "      <td>0.100812</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.973803</td>\n",
       "      <td>0.109072</td>\n",
       "      <td>0.892781</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>0.107117</td>\n",
       "      <td>0.093174</td>\n",
       "      <td>0.092319</td>\n",
       "      <td>0.110553</td>\n",
       "      <td>0.100812</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.210106</td>\n",
       "      <td>0.043877</td>\n",
       "      <td>0.782082</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>0.107117</td>\n",
       "      <td>0.093174</td>\n",
       "      <td>0.092319</td>\n",
       "      <td>0.110553</td>\n",
       "      <td>0.100812</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.095508</td>\n",
       "      <td>0.088096</td>\n",
       "      <td>0.776479</td>\n",
       "      <td>0.013298</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.85, 'countvect__min_df...</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>0.077149</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.072982</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.116178</td>\n",
       "      <td>0.107104</td>\n",
       "      <td>0.779601</td>\n",
       "      <td>0.009215</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>0.077149</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.072982</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.154454</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>0.777169</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.9, 'countvect__min_df'...</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>0.077149</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.072982</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.107284</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>0.783854</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>{'countvect__max_df': 0.8, 'countvect__min_df'...</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>0.077149</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.072982</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       10.432982      1.866859         0.662657        0.068872   \n",
       "8        9.090825      1.215975         0.566545        0.005077   \n",
       "24       8.839809      1.022437         0.558752        0.007119   \n",
       "16       8.854013      1.047872         0.566128        0.007749   \n",
       "18       8.173404      1.001998         0.549761        0.009949   \n",
       "26       8.149528      1.013364         0.542664        0.008855   \n",
       "2        8.699656      1.057683         0.618047        0.009326   \n",
       "10       8.169474      1.032815         0.554974        0.006630   \n",
       "12       7.220552      1.042784         0.546806        0.009530   \n",
       "28       7.159653      1.030859         0.537663        0.008782   \n",
       "20       7.181622      1.009740         0.536921        0.011758   \n",
       "4        8.027201      1.151801         0.621795        0.006342   \n",
       "22       6.691958      0.885304         0.525314        0.015363   \n",
       "14       6.691072      0.874775         0.531409        0.001909   \n",
       "30       6.691791      0.731539         0.543389        0.012296   \n",
       "6        6.744992      0.865440         0.529615        0.005578   \n",
       "1        8.724845      0.351676         0.942195        0.013466   \n",
       "17       7.983344      0.402603         0.857127        0.010479   \n",
       "9        8.045657      0.344047         0.863838        0.016539   \n",
       "25       7.975618      0.291010         0.847991        0.017757   \n",
       "3        7.184012      0.153570         0.904768        0.012956   \n",
       "27       6.430287      0.144647         0.793471        0.016718   \n",
       "11       6.488005      0.163254         0.804580        0.010491   \n",
       "19       6.478919      0.159535         0.812701        0.021175   \n",
       "13       6.277324      0.097248         0.792323        0.012496   \n",
       "21       6.217672      0.066639         0.790604        0.011824   \n",
       "5        6.973803      0.109072         0.892781        0.020018   \n",
       "29       6.210106      0.043877         0.782082        0.015400   \n",
       "23       6.095508      0.088096         0.776479        0.013298   \n",
       "7        6.116178      0.107104         0.779601        0.009215   \n",
       "15       6.154454      0.111620         0.777169        0.011570   \n",
       "31       6.107284      0.101653         0.783854        0.012008   \n",
       "\n",
       "   param_countvect__max_df param_countvect__min_df  \\\n",
       "0                     0.95                  0.0001   \n",
       "8                      0.9                  0.0001   \n",
       "24                     0.8                  0.0001   \n",
       "16                    0.85                  0.0001   \n",
       "18                    0.85                  0.0005   \n",
       "26                     0.8                  0.0005   \n",
       "2                     0.95                  0.0005   \n",
       "10                     0.9                  0.0005   \n",
       "12                     0.9                   0.001   \n",
       "28                     0.8                   0.001   \n",
       "20                    0.85                   0.001   \n",
       "4                     0.95                   0.001   \n",
       "22                    0.85                   0.002   \n",
       "14                     0.9                   0.002   \n",
       "30                     0.8                   0.002   \n",
       "6                     0.95                   0.002   \n",
       "1                     0.95                  0.0001   \n",
       "17                    0.85                  0.0001   \n",
       "9                      0.9                  0.0001   \n",
       "25                     0.8                  0.0001   \n",
       "3                     0.95                  0.0005   \n",
       "27                     0.8                  0.0005   \n",
       "11                     0.9                  0.0005   \n",
       "19                    0.85                  0.0005   \n",
       "13                     0.9                   0.001   \n",
       "21                    0.85                   0.001   \n",
       "5                     0.95                   0.001   \n",
       "29                     0.8                   0.001   \n",
       "23                    0.85                   0.002   \n",
       "7                     0.95                   0.002   \n",
       "15                     0.9                   0.002   \n",
       "31                     0.8                   0.002   \n",
       "\n",
       "   param_countvect__ngram_range  \\\n",
       "0                        (1, 1)   \n",
       "8                        (1, 1)   \n",
       "24                       (1, 1)   \n",
       "16                       (1, 1)   \n",
       "18                       (1, 1)   \n",
       "26                       (1, 1)   \n",
       "2                        (1, 1)   \n",
       "10                       (1, 1)   \n",
       "12                       (1, 1)   \n",
       "28                       (1, 1)   \n",
       "20                       (1, 1)   \n",
       "4                        (1, 1)   \n",
       "22                       (1, 1)   \n",
       "14                       (1, 1)   \n",
       "30                       (1, 1)   \n",
       "6                        (1, 1)   \n",
       "1                        (2, 2)   \n",
       "17                       (2, 2)   \n",
       "9                        (2, 2)   \n",
       "25                       (2, 2)   \n",
       "3                        (2, 2)   \n",
       "27                       (2, 2)   \n",
       "11                       (2, 2)   \n",
       "19                       (2, 2)   \n",
       "13                       (2, 2)   \n",
       "21                       (2, 2)   \n",
       "5                        (2, 2)   \n",
       "29                       (2, 2)   \n",
       "23                       (2, 2)   \n",
       "7                        (2, 2)   \n",
       "15                       (2, 2)   \n",
       "31                       (2, 2)   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'countvect__max_df': 0.95, 'countvect__min_df...           0.750159   \n",
       "8   {'countvect__max_df': 0.9, 'countvect__min_df'...           0.750159   \n",
       "24  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.750159   \n",
       "16  {'countvect__max_df': 0.85, 'countvect__min_df...           0.750159   \n",
       "18  {'countvect__max_df': 0.85, 'countvect__min_df...           0.737915   \n",
       "26  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.737915   \n",
       "2   {'countvect__max_df': 0.95, 'countvect__min_df...           0.737915   \n",
       "10  {'countvect__max_df': 0.9, 'countvect__min_df'...           0.737915   \n",
       "12  {'countvect__max_df': 0.9, 'countvect__min_df'...           0.724054   \n",
       "28  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.724054   \n",
       "20  {'countvect__max_df': 0.85, 'countvect__min_df...           0.724054   \n",
       "4   {'countvect__max_df': 0.95, 'countvect__min_df...           0.724054   \n",
       "22  {'countvect__max_df': 0.85, 'countvect__min_df...           0.689577   \n",
       "14  {'countvect__max_df': 0.9, 'countvect__min_df'...           0.689577   \n",
       "30  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.689577   \n",
       "6   {'countvect__max_df': 0.95, 'countvect__min_df...           0.689577   \n",
       "1   {'countvect__max_df': 0.95, 'countvect__min_df...           0.350339   \n",
       "17  {'countvect__max_df': 0.85, 'countvect__min_df...           0.350339   \n",
       "9   {'countvect__max_df': 0.9, 'countvect__min_df'...           0.350339   \n",
       "25  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.350339   \n",
       "3   {'countvect__max_df': 0.95, 'countvect__min_df...           0.161554   \n",
       "27  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.161554   \n",
       "11  {'countvect__max_df': 0.9, 'countvect__min_df'...           0.161554   \n",
       "19  {'countvect__max_df': 0.85, 'countvect__min_df...           0.161554   \n",
       "13  {'countvect__max_df': 0.9, 'countvect__min_df'...           0.100894   \n",
       "21  {'countvect__max_df': 0.85, 'countvect__min_df...           0.100894   \n",
       "5   {'countvect__max_df': 0.95, 'countvect__min_df...           0.100894   \n",
       "29  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.100894   \n",
       "23  {'countvect__max_df': 0.85, 'countvect__min_df...           0.070304   \n",
       "7   {'countvect__max_df': 0.95, 'countvect__min_df...           0.070304   \n",
       "15  {'countvect__max_df': 0.9, 'countvect__min_df'...           0.070304   \n",
       "31  {'countvect__max_df': 0.8, 'countvect__min_df'...           0.070304   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.764893           0.764932           0.745038   \n",
       "8            0.764893           0.764932           0.745038   \n",
       "24           0.764893           0.764932           0.745038   \n",
       "16           0.764893           0.764932           0.745038   \n",
       "18           0.755251           0.748811           0.730634   \n",
       "26           0.755251           0.748811           0.730634   \n",
       "2            0.755251           0.748811           0.730634   \n",
       "10           0.755251           0.748811           0.730634   \n",
       "12           0.736449           0.735216           0.712009   \n",
       "28           0.736449           0.735216           0.712009   \n",
       "20           0.736449           0.735216           0.712009   \n",
       "4            0.736449           0.735216           0.712009   \n",
       "22           0.711324           0.702912           0.678330   \n",
       "14           0.711324           0.702912           0.678330   \n",
       "30           0.711324           0.702912           0.678330   \n",
       "6            0.711324           0.702912           0.678330   \n",
       "1            0.362353           0.355993           0.366627   \n",
       "17           0.362353           0.355993           0.366627   \n",
       "9            0.362353           0.355993           0.366627   \n",
       "25           0.362353           0.355993           0.366627   \n",
       "3            0.181078           0.163475           0.161092   \n",
       "27           0.181078           0.163475           0.161092   \n",
       "11           0.181078           0.163475           0.161092   \n",
       "19           0.181078           0.163475           0.161092   \n",
       "13           0.107117           0.093174           0.092319   \n",
       "21           0.107117           0.093174           0.092319   \n",
       "5            0.107117           0.093174           0.092319   \n",
       "29           0.107117           0.093174           0.092319   \n",
       "23           0.077149           0.066152           0.029290   \n",
       "7            0.077149           0.066152           0.029290   \n",
       "15           0.077149           0.066152           0.029290   \n",
       "31           0.077149           0.066152           0.029290   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.757407         0.756486        0.007924                1  \n",
       "8            0.757407         0.756486        0.007924                1  \n",
       "24           0.757407         0.756486        0.007924                1  \n",
       "16           0.757407         0.756486        0.007924                1  \n",
       "18           0.742808         0.743084        0.008516                5  \n",
       "26           0.742808         0.743084        0.008516                5  \n",
       "2            0.742808         0.743084        0.008516                5  \n",
       "10           0.742808         0.743084        0.008516                5  \n",
       "12           0.730004         0.727547        0.008919                9  \n",
       "28           0.730004         0.727547        0.008919                9  \n",
       "20           0.730004         0.727547        0.008919                9  \n",
       "4            0.730004         0.727547        0.008919                9  \n",
       "22           0.701530         0.696734        0.011524               13  \n",
       "14           0.701530         0.696734        0.011524               13  \n",
       "30           0.701530         0.696734        0.011524               13  \n",
       "6            0.701530         0.696734        0.011524               13  \n",
       "1            0.368943         0.360851        0.006859               17  \n",
       "17           0.368943         0.360851        0.006859               17  \n",
       "9            0.368943         0.360851        0.006859               17  \n",
       "25           0.368943         0.360851        0.006859               17  \n",
       "3            0.176331         0.168706        0.008339               21  \n",
       "27           0.176331         0.168706        0.008339               21  \n",
       "11           0.176331         0.168706        0.008339               21  \n",
       "19           0.176331         0.168706        0.008339               21  \n",
       "13           0.110553         0.100812        0.007282               25  \n",
       "21           0.110553         0.100812        0.007282               25  \n",
       "5            0.110553         0.100812        0.007282               25  \n",
       "29           0.110553         0.100812        0.007282               25  \n",
       "23           0.072982         0.063175        0.017317               29  \n",
       "7            0.072982         0.063175        0.017317               29  \n",
       "15           0.072982         0.063175        0.017317               29  \n",
       "31           0.072982         0.063175        0.017317               29  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank all models based on the f1-score metric\n",
    "\n",
    "model_bow_results = pd.DataFrame(model_bow.cv_results_).sort_values(by='mean_test_score', ascending=False)\n",
    "model_bow_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16a8e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('countvect', CountVectorizer(max_df=0.95, min_df=0.0001)), ('clf', LogisticRegression(max_iter=20000))]\n"
     ]
    }
   ],
   "source": [
    "# Best model based on f1-score metric\n",
    "\n",
    "print(model_bow.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61836ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     28659\n",
      "           1       0.85      0.70      0.77      3245\n",
      "\n",
      "    accuracy                           0.96     31904\n",
      "   macro avg       0.91      0.84      0.87     31904\n",
      "weighted avg       0.95      0.96      0.95     31904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the classification metrics for the test set\n",
    "\n",
    "y_pred_bow = model_bow.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "06ad2d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    114633\n",
      "           1       0.95      0.76      0.84     12980\n",
      "\n",
      "    accuracy                           0.97    127613\n",
      "   macro avg       0.96      0.88      0.91    127613\n",
      "weighted avg       0.97      0.97      0.97    127613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the classification metrics for the test set\n",
    "\n",
    "y_pred_train_bow = model_bow.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9225a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>10.432982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.662657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.756486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 0\n",
       "mean_fit_time                                            10.432982\n",
       "mean_score_time                                           0.662657\n",
       "params           {'countvect__max_df': 0.95, 'countvect__min_df...\n",
       "mean_test_score                                           0.756486"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of the results including fit time and score\n",
    "\n",
    "bow = model_bow_results[['mean_fit_time', 'mean_score_time', 'params', 'mean_test_score']].head(1).transpose()\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e71b3",
   "metadata": {},
   "source": [
    "### 3.2. Normalized TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c38469",
   "metadata": {},
   "source": [
    "The next text vectorization model that I use in this project is TF-IDF algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a1ddc",
   "metadata": {},
   "source": [
    "### 3.2.1. Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc2090a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X = df['clean_text']\n",
    "y = df['toxic_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd1f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to perform train/test split and then fitting and transforming using TF-IDF\n",
    "\n",
    "def tfidfvectorize(X, y, max_df=0.95, min_df=0.001, ngram_range=(1,1), norm='l2', \n",
    "                   use_idf=True, smooth_idf=True, sublinear_tf=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                                stratify=y, random_state=1)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.001, ngram_range=(1,1), norm='l2', \n",
    "                   use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    \n",
    "    Dict_count = vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train_vec = Dict_count.transform(X_train).toarray()\n",
    "    X_test_vec = Dict_count.transform(X_test).toarray()\n",
    "    \n",
    "    X_train_df = pd.DataFrame(data=X_train_vec, columns=vectorizer.get_feature_names())\n",
    "    X_test_df = pd.DataFrame(data=X_test_vec, columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    y_train_df = y_train\n",
    "    y_test_df = y_test\n",
    "    \n",
    "    print('Number of labels in the train set: \\n', y_train.value_counts(normalize=True))\n",
    "    print('-----------------------------------')\n",
    "    print('Number of labels in the test set: \\n', y_test.value_counts(normalize=True))\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    X_train = np.array(X_train_df)\n",
    "    X_test = np.array(X_test_df)\n",
    "    y_train = np.array(y_train_df)\n",
    "    y_test = np.array(y_test_df)\n",
    "    \n",
    "    print(\"X train type and shape: \", type(X_train), X_train.shape)\n",
    "    print(\"y train type and shape: \", type(y_train), y_train.shape)\n",
    "    \n",
    "    print(\"X test type and shape: \", type(X_test), X_test.shape)\n",
    "    print(\"y test type and shape: \", type(y_test), y_test.shape)\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d4b7172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in the train set: \n",
      " 0    0.898286\n",
      "1    0.101714\n",
      "Name: toxic_type, dtype: float64\n",
      "-----------------------------------\n",
      "Number of labels in the test set: \n",
      " 0    0.898289\n",
      "1    0.101711\n",
      "Name: toxic_type, dtype: float64\n",
      "-----------------------------------\n",
      "X train type and shape:  <class 'numpy.ndarray'> (127613, 2958)\n",
      "y train type and shape:  <class 'numpy.ndarray'> (127613,)\n",
      "X test type and shape:  <class 'numpy.ndarray'> (31904, 2958)\n",
      "y test type and shape:  <class 'numpy.ndarray'> (31904,)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TFIDF vectorizer\n",
    "\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = tfidfvectorize(X, y, max_df=0.95, min_df=0.001, ngram_range=(1,1), \n",
    "                                                 norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6faf95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=20000, random_state=1)\n",
    "clf_model = clf.fit(X_train_tf, y_train_tf)\n",
    "\n",
    "y_pred_train_tf = clf_model.predict(X_train_tf)\n",
    "\n",
    "y_pred_test_tf = clf_model.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2183bec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28659\n",
      "           1       0.91      0.63      0.75      3245\n",
      "\n",
      "    accuracy                           0.96     31904\n",
      "   macro avg       0.94      0.81      0.86     31904\n",
      "weighted avg       0.96      0.96      0.95     31904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the classification metrics for the test set\n",
    "\n",
    "print(\"Test Classification Report\")\n",
    "print(classification_report(y_test_tf, y_pred_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4335818b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    114633\n",
      "           1       0.92      0.63      0.75     12980\n",
      "\n",
      "    accuracy                           0.96    127613\n",
      "   macro avg       0.94      0.81      0.86    127613\n",
      "weighted avg       0.96      0.96      0.95    127613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the classification metrics for the train set\n",
    "\n",
    "print(\"Test Classification Report\")\n",
    "print(classification_report(y_train_tf, y_pred_train_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f497b55",
   "metadata": {},
   "source": [
    "### 3.2.2. TF-IDF Vectorization - Parameters Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1634c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df['toxic_type']\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863a1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TF-IDF and Logistic Regression models with different parameters\n",
    "\n",
    "pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LogisticRegression(solver='lbfgs', max_iter=20000))])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.95, 0.9, 0.85, 0.8),\n",
    "    'tfidf__min_df': (0.0001, 0.0005, 0.001, 0.002),\n",
    "    'tfidf__ngram_range': ((1,1), (2,2)),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__smooth_idf': (True, False),\n",
    "    'tfidf__sublinear_tf': (True, False)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='f1')\n",
    "model_tf = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aee2e20f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tfidf__max_df</th>\n",
       "      <th>param_tfidf__min_df</th>\n",
       "      <th>param_tfidf__ngram_range</th>\n",
       "      <th>param_tfidf__norm</th>\n",
       "      <th>param_tfidf__smooth_idf</th>\n",
       "      <th>param_tfidf__sublinear_tf</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3.052689</td>\n",
       "      <td>0.043315</td>\n",
       "      <td>0.512436</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'tfidf__max_df': 0.9, 'tfidf__min_df': 0.0005...</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.759591</td>\n",
       "      <td>0.746905</td>\n",
       "      <td>0.730402</td>\n",
       "      <td>0.748690</td>\n",
       "      <td>0.745164</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>3.061287</td>\n",
       "      <td>0.033217</td>\n",
       "      <td>0.507843</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'tfidf__max_df': 0.85, 'tfidf__min_df': 0.000...</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.759591</td>\n",
       "      <td>0.746905</td>\n",
       "      <td>0.730402</td>\n",
       "      <td>0.748690</td>\n",
       "      <td>0.745164</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3.311951</td>\n",
       "      <td>0.125871</td>\n",
       "      <td>0.561447</td>\n",
       "      <td>0.008321</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'tfidf__max_df': 0.95, 'tfidf__min_df': 0.000...</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.759591</td>\n",
       "      <td>0.746905</td>\n",
       "      <td>0.730402</td>\n",
       "      <td>0.748690</td>\n",
       "      <td>0.745164</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3.381707</td>\n",
       "      <td>0.076519</td>\n",
       "      <td>0.572371</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'tfidf__max_df': 0.95, 'tfidf__min_df': 0.000...</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.759591</td>\n",
       "      <td>0.746905</td>\n",
       "      <td>0.730402</td>\n",
       "      <td>0.748690</td>\n",
       "      <td>0.745164</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>3.041076</td>\n",
       "      <td>0.120493</td>\n",
       "      <td>0.508623</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'tfidf__max_df': 0.8, 'tfidf__min_df': 0.0005...</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.759591</td>\n",
       "      <td>0.746905</td>\n",
       "      <td>0.730402</td>\n",
       "      <td>0.748690</td>\n",
       "      <td>0.745164</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>5.244102</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.689252</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'tfidf__max_df': 0.8, 'tfidf__min_df': 0.002,...</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>5.337852</td>\n",
       "      <td>0.054408</td>\n",
       "      <td>0.694013</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'tfidf__max_df': 0.9, 'tfidf__min_df': 0.002,...</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>5.277293</td>\n",
       "      <td>0.034150</td>\n",
       "      <td>0.685507</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'tfidf__max_df': 0.85, 'tfidf__min_df': 0.002...</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5.350179</td>\n",
       "      <td>0.058524</td>\n",
       "      <td>0.709494</td>\n",
       "      <td>0.018204</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'tfidf__max_df': 0.95, 'tfidf__min_df': 0.002...</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>5.270117</td>\n",
       "      <td>0.076613</td>\n",
       "      <td>0.691701</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'tfidf__max_df': 0.85, 'tfidf__min_df': 0.002...</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.052926</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "172       3.052689      0.043315         0.512436        0.010285   \n",
       "300       3.061287      0.033217         0.507843        0.004753   \n",
       "40        3.311951      0.125871         0.561447        0.008321   \n",
       "44        3.381707      0.076519         0.572371        0.019592   \n",
       "424       3.041076      0.120493         0.508623        0.004067   \n",
       "..             ...           ...              ...             ...   \n",
       "497       5.244102      0.083984         0.689252        0.007039   \n",
       "247       5.337852      0.054408         0.694013        0.004190   \n",
       "371       5.277293      0.034150         0.685507        0.006322   \n",
       "113       5.350179      0.058524         0.709494        0.018204   \n",
       "373       5.270117      0.076613         0.691701        0.012542   \n",
       "\n",
       "    param_tfidf__max_df param_tfidf__min_df param_tfidf__ngram_range  \\\n",
       "172                 0.9              0.0005                   (1, 1)   \n",
       "300                0.85              0.0005                   (1, 1)   \n",
       "40                 0.95              0.0005                   (1, 1)   \n",
       "44                 0.95              0.0005                   (1, 1)   \n",
       "424                 0.8              0.0005                   (1, 1)   \n",
       "..                  ...                 ...                      ...   \n",
       "497                 0.8               0.002                   (2, 2)   \n",
       "247                 0.9               0.002                   (2, 2)   \n",
       "371                0.85               0.002                   (2, 2)   \n",
       "113                0.95               0.002                   (2, 2)   \n",
       "373                0.85               0.002                   (2, 2)   \n",
       "\n",
       "    param_tfidf__norm param_tfidf__smooth_idf param_tfidf__sublinear_tf  \\\n",
       "172                l2                   False                      True   \n",
       "300                l2                   False                      True   \n",
       "40                 l2                    True                      True   \n",
       "44                 l2                   False                      True   \n",
       "424                l2                    True                      True   \n",
       "..                ...                     ...                       ...   \n",
       "497                l1                    True                      True   \n",
       "247                l1                   False                     False   \n",
       "371                l1                    True                     False   \n",
       "113                l1                    True                      True   \n",
       "373                l1                   False                      True   \n",
       "\n",
       "    param_tfidf__use_idf                                             params  \\\n",
       "172                 True  {'tfidf__max_df': 0.9, 'tfidf__min_df': 0.0005...   \n",
       "300                 True  {'tfidf__max_df': 0.85, 'tfidf__min_df': 0.000...   \n",
       "40                  True  {'tfidf__max_df': 0.95, 'tfidf__min_df': 0.000...   \n",
       "44                  True  {'tfidf__max_df': 0.95, 'tfidf__min_df': 0.000...   \n",
       "424                 True  {'tfidf__max_df': 0.8, 'tfidf__min_df': 0.0005...   \n",
       "..                   ...                                                ...   \n",
       "497                False  {'tfidf__max_df': 0.8, 'tfidf__min_df': 0.002,...   \n",
       "247                False  {'tfidf__max_df': 0.9, 'tfidf__min_df': 0.002,...   \n",
       "371                False  {'tfidf__max_df': 0.85, 'tfidf__min_df': 0.002...   \n",
       "113                False  {'tfidf__max_df': 0.95, 'tfidf__min_df': 0.002...   \n",
       "373                False  {'tfidf__max_df': 0.85, 'tfidf__min_df': 0.002...   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "172           0.740230           0.759591           0.746905   \n",
       "300           0.740230           0.759591           0.746905   \n",
       "40            0.740230           0.759591           0.746905   \n",
       "44            0.740230           0.759591           0.746905   \n",
       "424           0.740230           0.759591           0.746905   \n",
       "..                 ...                ...                ...   \n",
       "497           0.059524           0.059502           0.052926   \n",
       "247           0.059524           0.059502           0.052926   \n",
       "371           0.059524           0.059502           0.052926   \n",
       "113           0.059524           0.059502           0.052926   \n",
       "373           0.059524           0.059502           0.052926   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "172           0.730402           0.748690         0.745164        0.009653   \n",
       "300           0.730402           0.748690         0.745164        0.009653   \n",
       "40            0.730402           0.748690         0.745164        0.009653   \n",
       "44            0.730402           0.748690         0.745164        0.009653   \n",
       "424           0.730402           0.748690         0.745164        0.009653   \n",
       "..                 ...                ...              ...             ...   \n",
       "497           0.021220           0.056569         0.049948        0.014567   \n",
       "247           0.021220           0.056569         0.049948        0.014567   \n",
       "371           0.021220           0.056569         0.049948        0.014567   \n",
       "113           0.021220           0.056569         0.049948        0.014567   \n",
       "373           0.021220           0.056569         0.049948        0.014567   \n",
       "\n",
       "     rank_test_score  \n",
       "172                1  \n",
       "300                1  \n",
       "40                 1  \n",
       "44                 1  \n",
       "424                1  \n",
       "..               ...  \n",
       "497              497  \n",
       "247              497  \n",
       "371              497  \n",
       "113              497  \n",
       "373              497  \n",
       "\n",
       "[512 rows x 20 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank the results based on the f1-score metric\n",
    "\n",
    "model_tf_results = pd.DataFrame(model_tf.cv_results_).sort_values(by='mean_test_score', ascending=False)\n",
    "model_tf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b04879a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28659\n",
      "           1       0.92      0.66      0.77      3245\n",
      "\n",
      "    accuracy                           0.96     31904\n",
      "   macro avg       0.94      0.83      0.87     31904\n",
      "weighted avg       0.96      0.96      0.96     31904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for the test set\n",
    "\n",
    "y_pred_tf = model_tf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "113f3f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98    114633\n",
      "           1       0.93      0.66      0.77     12980\n",
      "\n",
      "    accuracy                           0.96    127613\n",
      "   macro avg       0.95      0.83      0.87    127613\n",
      "weighted avg       0.96      0.96      0.96    127613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for the train set\n",
    "\n",
    "y_pred_train_tf = model_tf.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8faa3429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.95, min_df=0.0005, sublinear_tf=True)\n"
     ]
    }
   ],
   "source": [
    "# The best model based on the f1-score metric\n",
    "\n",
    "best_tfidf_model = model_tf.best_estimator_.steps[0][1]\n",
    "print(best_tfidf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66ab6ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>172</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>3.052689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.512436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'tfidf__max_df': 0.9, 'tfidf__min_df': 0.0005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.745164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               172\n",
       "mean_fit_time                                             3.052689\n",
       "mean_score_time                                           0.512436\n",
       "params           {'tfidf__max_df': 0.9, 'tfidf__min_df': 0.0005...\n",
       "mean_test_score                                           0.745164"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of the results including fit time and score\n",
    "\n",
    "tf = model_tf_results[['mean_fit_time', 'mean_score_time', 'params', 'mean_test_score']].head(1).transpose()\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6471ab",
   "metadata": {},
   "source": [
    "### 3.3. Summary of Text Vecorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd5df0",
   "metadata": {},
   "source": [
    "The results for BOW and TF-IDF show that the perfromance of both model (in terms of f1-score metric) is very close. However, TF-IDF model performs better in terms of fit-time. The TF-IDF model is three times faster than the BOW model. Therefore, the TF-IDF technique is selected for the rest of modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03962641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW Terms Frequency</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>10.432982</td>\n",
       "      <td>3.052689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.662657</td>\n",
       "      <td>0.512436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'countvect__max_df': 0.95, 'countvect__min_df': 0.0001, 'countvect__ngram_range': (1, 1)}</td>\n",
       "      <td>{'tfidf__max_df': 0.9, 'tfidf__min_df': 0.0005, 'tfidf__ngram_range': (1, 1), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__sublinear_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.756486</td>\n",
       "      <td>0.745164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        BOW Terms Frequency                                                                                                                                                 TF-IDF\n",
       "mean_fit_time                                                                                     10.432982                                                                                                                                               3.052689\n",
       "mean_score_time                                                                                    0.662657                                                                                                                                               0.512436\n",
       "params           {'countvect__max_df': 0.95, 'countvect__min_df': 0.0001, 'countvect__ngram_range': (1, 1)}  {'tfidf__max_df': 0.9, 'tfidf__min_df': 0.0005, 'tfidf__ngram_range': (1, 1), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__sublinear_t...\n",
       "mean_test_score                                                                                    0.756486                                                                                                                                               0.745164"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sum = pd.concat([bow, tf], axis=1)\n",
    "vec_sum.columns = ['BOW Terms Frequency', 'TF-IDF']\n",
    "vec_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c5c92da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best vectorization model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_df=0.95, min_df=0.0005,\n",
       "                                 sublinear_tf=True)),\n",
       "                ('clf', LogisticRegression(max_iter=20000))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The best vectorization model:')\n",
    "model_tf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137786c0",
   "metadata": {},
   "source": [
    "### 4. Evaluate Imbalanced Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd62485",
   "metadata": {},
   "source": [
    "As explained earlier, this dataset is imbalanced. The toxic-labeled comments are 10 percent of the non-toxic comments. This imbalance results in a small recall. In order to address the imbalance issue, I use oversamling, undersampling and combined techniques. Resampling techniques in conjunction with TF-IDF and GridSearch is employed to investigate the most optimal technique with the optimum sampling ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cf8e6",
   "metadata": {},
   "source": [
    "### 4.1. Oversampling Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11769c",
   "metadata": {},
   "source": [
    "### 4.1.1. Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07354009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df['toxic_type']\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "344f2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF text vectorization technique in conjunction with Random Oversampling and Logistic Regression techniques\n",
    "\n",
    "pipeline = imbpipeline([('tfidf', TfidfVectorizer(max_df=0.95,\n",
    "                            min_df=0.0005, sublinear_tf=True)),\n",
    "                     ('over', RandomOverSampler(random_state=0)),\n",
    "                     ('clf', LogisticRegression(solver='lbfgs', max_iter=20000))])\n",
    "\n",
    "parameters = {\n",
    "    'over__sampling_strategy': (1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='f1')\n",
    "model_R1 = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a8ee084",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_over__sampling_strategy</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.487857</td>\n",
       "      <td>0.214337</td>\n",
       "      <td>0.631202</td>\n",
       "      <td>0.022210</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'over__sampling_strategy': 0.3}</td>\n",
       "      <td>0.771588</td>\n",
       "      <td>0.782678</td>\n",
       "      <td>0.774142</td>\n",
       "      <td>0.760073</td>\n",
       "      <td>0.775526</td>\n",
       "      <td>0.772801</td>\n",
       "      <td>0.007352</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.763166</td>\n",
       "      <td>0.164054</td>\n",
       "      <td>0.627233</td>\n",
       "      <td>0.036916</td>\n",
       "      <td>0.4</td>\n",
       "      <td>{'over__sampling_strategy': 0.4}</td>\n",
       "      <td>0.768094</td>\n",
       "      <td>0.769608</td>\n",
       "      <td>0.768848</td>\n",
       "      <td>0.760953</td>\n",
       "      <td>0.770489</td>\n",
       "      <td>0.767598</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.822719</td>\n",
       "      <td>0.647701</td>\n",
       "      <td>0.696380</td>\n",
       "      <td>0.089506</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'over__sampling_strategy': 0.5}</td>\n",
       "      <td>0.758445</td>\n",
       "      <td>0.766158</td>\n",
       "      <td>0.761238</td>\n",
       "      <td>0.754214</td>\n",
       "      <td>0.758218</td>\n",
       "      <td>0.759655</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.588342</td>\n",
       "      <td>0.360693</td>\n",
       "      <td>0.587250</td>\n",
       "      <td>0.015516</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'over__sampling_strategy': 0.6}</td>\n",
       "      <td>0.748862</td>\n",
       "      <td>0.751494</td>\n",
       "      <td>0.748798</td>\n",
       "      <td>0.742684</td>\n",
       "      <td>0.748458</td>\n",
       "      <td>0.748059</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.269987</td>\n",
       "      <td>0.423330</td>\n",
       "      <td>0.597438</td>\n",
       "      <td>0.051972</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'over__sampling_strategy': 0.7}</td>\n",
       "      <td>0.738326</td>\n",
       "      <td>0.742964</td>\n",
       "      <td>0.735081</td>\n",
       "      <td>0.731716</td>\n",
       "      <td>0.735640</td>\n",
       "      <td>0.736745</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.677800</td>\n",
       "      <td>0.349030</td>\n",
       "      <td>0.645635</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'over__sampling_strategy': 0.8}</td>\n",
       "      <td>0.728114</td>\n",
       "      <td>0.728239</td>\n",
       "      <td>0.727303</td>\n",
       "      <td>0.718440</td>\n",
       "      <td>0.724768</td>\n",
       "      <td>0.725373</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.425109</td>\n",
       "      <td>0.342835</td>\n",
       "      <td>0.634329</td>\n",
       "      <td>0.035885</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'over__sampling_strategy': 0.9}</td>\n",
       "      <td>0.717916</td>\n",
       "      <td>0.716442</td>\n",
       "      <td>0.718581</td>\n",
       "      <td>0.708757</td>\n",
       "      <td>0.716045</td>\n",
       "      <td>0.715548</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.400107</td>\n",
       "      <td>0.176558</td>\n",
       "      <td>0.632666</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>1</td>\n",
       "      <td>{'over__sampling_strategy': 1}</td>\n",
       "      <td>0.707425</td>\n",
       "      <td>0.705136</td>\n",
       "      <td>0.707614</td>\n",
       "      <td>0.701428</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>0.705908</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_over__sampling_strategy                            params  split0_test_score  split1_test_score  split2_test_score  split3_test_score  split4_test_score  mean_test_score  std_test_score  rank_test_score\n",
       "7       4.487857      0.214337         0.631202        0.022210                           0.3  {'over__sampling_strategy': 0.3}           0.771588           0.782678           0.774142           0.760073           0.775526         0.772801        0.007352                1\n",
       "6       3.763166      0.164054         0.627233        0.036916                           0.4  {'over__sampling_strategy': 0.4}           0.768094           0.769608           0.768848           0.760953           0.770489         0.767598        0.003416                2\n",
       "5       4.822719      0.647701         0.696380        0.089506                           0.5  {'over__sampling_strategy': 0.5}           0.758445           0.766158           0.761238           0.754214           0.758218         0.759655        0.003948                3\n",
       "4       4.588342      0.360693         0.587250        0.015516                           0.6  {'over__sampling_strategy': 0.6}           0.748862           0.751494           0.748798           0.742684           0.748458         0.748059        0.002900                4\n",
       "3       4.269987      0.423330         0.597438        0.051972                           0.7  {'over__sampling_strategy': 0.7}           0.738326           0.742964           0.735081           0.731716           0.735640         0.736745        0.003754                5\n",
       "2       4.677800      0.349030         0.645635        0.029531                           0.8  {'over__sampling_strategy': 0.8}           0.728114           0.728239           0.727303           0.718440           0.724768         0.725373        0.003685                6\n",
       "1       5.425109      0.342835         0.634329        0.035885                           0.9  {'over__sampling_strategy': 0.9}           0.717916           0.716442           0.718581           0.708757           0.716045         0.715548        0.003520                7\n",
       "0       5.400107      0.176558         0.632666        0.015415                             1    {'over__sampling_strategy': 1}           0.707425           0.705136           0.707614           0.701428           0.707937         0.705908        0.002449                8"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank the model based on the f1-score metric\n",
    "\n",
    "model_R1_results = pd.DataFrame(model_R1.cv_results_).sort_values(by='mean_test_score', ascending=False)\n",
    "model_R1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a3b3ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     28659\n",
      "           1       0.81      0.77      0.79      3245\n",
      "\n",
      "    accuracy                           0.96     31904\n",
      "   macro avg       0.89      0.87      0.88     31904\n",
      "weighted avg       0.96      0.96      0.96     31904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for the test set\n",
    "\n",
    "y_pred_R1 = model_R1.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_R1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07a69c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    114633\n",
      "           1       0.82      0.79      0.81     12980\n",
      "\n",
      "    accuracy                           0.96    127613\n",
      "   macro avg       0.90      0.89      0.89    127613\n",
      "weighted avg       0.96      0.96      0.96    127613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for the train set\n",
    "\n",
    "y_pred_train_R1 = model_R1.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train_R1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a1c42a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>4.487857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.631202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'over__sampling_strategy': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.772801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                7\n",
       "mean_fit_time                            4.487857\n",
       "mean_score_time                          0.631202\n",
       "params           {'over__sampling_strategy': 0.3}\n",
       "mean_test_score                          0.772801"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of the results including fit time and score\n",
    "\n",
    "R1_est = model_R1_results[['mean_fit_time', 'mean_score_time', 'params', 'mean_test_score']].head(1).transpose()\n",
    "R1_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bcc65",
   "metadata": {},
   "source": [
    "Oversampling decreased the precision while increased the recall and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c75085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
